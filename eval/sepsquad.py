import torch
import os
import json
from misc.util import timed
from data.dataloader import datapath
from data.support import RawSample, BeforeSubSeqMasker
import torch.nn as nn

# sepsquad is generated by file eval/sepsquad_prep.py, and then stored in
# dataset folder as it has both train and eval parts

squad_path = f"{datapath}/mysquad"

def get_sepsquad_QAs(lm, subset="validation"):
    assert subset in ["validation", "test"], subset
    if not hasattr(lm, "sepsquad_dl_cache"):
        lm.sepsquad_dl_cache = {}
    if subset not in lm.sepsquad_dl_cache:
        QAs = {}
        qmasker = BeforeSubSeqMasker("] A: [")
        for u in ["known", "unknown"]:
            with open(f"{squad_path}/{subset}_{u}_QAs.txt", "r") as f:
                QAs[u] = [RawSample(s.replace("\n",""), note=f"{u}_QA",
                          target_masker=qmasker)
                          for s in f.readlines()]
        batch_size = 16
        shuffle = False  # important for aligning perplexities with questions
        QAs = {u: lm._prepare_dl(qs, batch_size, shuffle)
               for u, qs in QAs.items()}
        lm.sepsquad_dl_cache[subset] = QAs
    return lm.sepsquad_dl_cache[subset]


def get_QA_results(lm, dl_qas):
    loss_fn = nn.CrossEntropyLoss(reduction="none",
                                   ignore_index=lm.ignore_index)
    answer_total_losses = []
    answer_token_accs = []
    answer_exact_matches = []
    for batch in dl_qas:
        xyz = lm.get_batch_xyz(batch)
        # y filled with lm.ignore_index wherever not answer
        # batch["target_mask"] gives answer positions
        # y, z, target mask aligned on positions
        y, z, tm = xyz["y"], xyz["z"], batch["target_mask"]
        # y, target mask shape: batch size x seq len
        # z shape: batch size x seq len x vocab size
        celosses = loss_fn(z.reshape(-1, z.shape[-1]),
                           y.reshape(-1)).view(y.shape)
        # batch size x seq len
        celosses = torch.where(tm.bool(), 0, celosses)
        # 0 where not relevant
        answer_total_losses += celosses.sum(dim=-1).tolist()  # batch size
        answer_nmatches = torch.logical_and(torch.logical_not(tm.bool()),
                                            z.argmax(dim=-1) == y).sum(dim=-1)
        answer_ntokens = tm.sum(dim=-1)
        answer_token_accs += (answer_nmatches / answer_ntokens).tolist()
        answer_exact_matches += (answer_nmatches == answer_ntokens).tolist()
    def mean(lst):
        return sum(lst) / len(lst)
    things = {"total-loss": answer_total_losses,
              "token-accs": answer_token_accs,
              "exact-matches": answer_exact_matches}
    return {k: mean(v) for k, v in things.items()}


@timed
def sepsquad_eval(lm, subset="validation"):
    QAs = get_sepsquad_QAs(lm, subset)
    res = {}
    for u, qs in QAs.items():
        res.update({f"{subset}-{u}-qs-{m}": v
                    for m, v in get_QA_results(lm, qs).items()})
    return res  # dict of title, value
